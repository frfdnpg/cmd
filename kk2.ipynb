{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, QED\n",
    "from tensorflow.contrib.keras import preprocessing\n",
    "\n",
    "\n",
    "def get_property(smi):\n",
    "\n",
    "    try:\n",
    "        mol=Chem.MolFromSmiles(smi) \n",
    "        property = [Descriptors.ExactMolWt(mol), Descriptors.MolLogP(mol), QED.qed(mol)]\n",
    "        \n",
    "    except:\n",
    "        property = 'invalid'\n",
    "           \n",
    "    return property\n",
    "    \n",
    "\n",
    "def canonocalize(smi):\n",
    "\n",
    "    return Chem.MolToSmiles(Chem.MolFromSmiles(smi))\n",
    "\n",
    "\n",
    "def vectorize(list_input, char_set):\n",
    "\n",
    "    one_hot = np.zeros((list_input.shape[0], list_input.shape[1]+4, len(char_set)), dtype=np.int32)\n",
    "\n",
    "    for si, ss in enumerate(list_input):\n",
    "        for cj, cc in enumerate(ss):\n",
    "            one_hot[si,cj+1,cc] = 1\n",
    "\n",
    "        one_hot[si,-1,0] = 1\n",
    "        one_hot[si,-2,0] = 1\n",
    "        one_hot[si,-3,0] = 1\n",
    "\n",
    "    return one_hot[:,0:-1,:], one_hot[:,1:,:]\n",
    "\n",
    "\n",
    "def smiles_to_seq(smiles, char_set):\n",
    "\n",
    "    char_to_int = dict((c,i) for i,c in enumerate(char_set))\n",
    "    \n",
    "    list_seq=[]\n",
    "    for s in smiles:\n",
    "        seq=[]                \n",
    "        j=0\n",
    "        while j<len(s):\n",
    "            if j<len(s)-1 and s[j:j+2] in char_set:\n",
    "                seq.append(char_to_int[s[j:j+2]])\n",
    "                j=j+2\n",
    "    \n",
    "            elif s[j] in char_set:\n",
    "                seq.append(char_to_int[s[j]])\n",
    "                j=j+1\n",
    "    \n",
    "        list_seq.append(seq)\n",
    "    \n",
    "    list_seq = preprocessing.sequence.pad_sequences(list_seq, padding='post')\n",
    "    \n",
    "    return list_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, seqlen_x, dim_x, dim_y, dim_z=100, dim_h=250, n_hidden=3, batch_size=200, beta=10000., char_set=[' ']):\n",
    "\n",
    "        self.seqlen_x, self.dim_x, self.dim_y, self.dim_z, self.dim_h, self.n_hidden, self.batch_size = seqlen_x, dim_x, dim_y, dim_z, dim_h, n_hidden, batch_size\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.char_to_int = dict((c,i) for i,c in enumerate(char_set))\n",
    "        self.int_to_char = dict((i,c) for i,c in enumerate(char_set))\n",
    "        \n",
    "        self.G = tf.Graph()\n",
    "        self.G.as_default()\n",
    "\n",
    "        ## variables for labeled data\n",
    "        self.x_L = tf.placeholder(tf.float32, [None, self.seqlen_x, self.dim_x])\n",
    "        self.xs_L = tf.placeholder(tf.float32, [None, self.seqlen_x, self.dim_x])\n",
    "        self.y_L = tf.placeholder(tf.float32, [None, self.dim_y])\n",
    "\n",
    "        ## functions for labeled data\n",
    "        self.classifier_L_out = self._rnnpredictor(self.x_L, self.dim_h, 2*self.dim_y, reuse = False)\n",
    "        self.y_L_mu, self.y_L_lsgms = tf.split(self.classifier_L_out, [self.dim_y, self.dim_y], 1)\n",
    "        self.y_L_sample = self._draw_sample(self.y_L_mu, self.y_L_lsgms)\n",
    "\n",
    "        self.encoder_L_out = self._rnnencoder(self.x_L, self.y_L, self.dim_h, 2*self.dim_z, reuse = False)\n",
    "        self.z_L_mu, self.z_L_lsgms = tf.split(self.encoder_L_out, [self.dim_z, self.dim_z], 1)\n",
    "        self.z_L_sample = self._draw_sample(self.z_L_mu, self.z_L_lsgms)\n",
    "\n",
    "        self.decoder_L_out = self._rnndecoder(self.xs_L, tf.concat([self.z_L_sample, self.y_L], 1), self.dim_h, self.dim_x, reuse = False)\n",
    "        self.x_L_recon = tf.nn.softmax(self.decoder_L_out)\n",
    "\n",
    "        self.decoder_DL_out = self._rnndecoder(self.xs_L, tf.concat([self.z_L_mu, self.y_L], 1), self.dim_h, self.dim_x, reuse = True)\n",
    "        self.x_DL_recon = tf.nn.softmax(self.decoder_DL_out)\n",
    "\n",
    "        self.z_G = tf.placeholder(tf.float32, [None, dim_z])\n",
    "        self.decoder_G_out = self._rnndecoder(self.xs_L, tf.concat([self.z_G, self.y_L], 1), self.dim_h, self.dim_x, reuse = True)\n",
    "        self.x_G_recon = tf.nn.softmax(self.decoder_G_out)\n",
    "\n",
    "\n",
    "        ## variables for unlabeled data\n",
    "        self.x_U = tf.placeholder(tf.float32, [None, self.seqlen_x, self.dim_x])\n",
    "        self.xs_U = tf.placeholder(tf.float32, [None, self.seqlen_x, self.dim_x])\n",
    "\n",
    "        ## functions for unlabeled data\n",
    "        self.classifier_U_out = self._rnnpredictor(self.x_U, self.dim_h, 2*self.dim_y, reuse = True)\n",
    "        self.y_U_mu, self.y_U_lsgms = tf.split(self.classifier_U_out, [self.dim_y, self.dim_y], 1)\n",
    "        self.y_U_sample = self._draw_sample(self.y_U_mu, self.y_U_lsgms)\n",
    "\n",
    "        self.encoder_U_out = self._rnnencoder(self.x_U, self.y_U_sample, self.dim_h, 2*self.dim_z, reuse = True)\n",
    "        self.z_U_mu, self.z_U_lsgms = tf.split(self.encoder_U_out, [self.dim_z, self.dim_z], 1)\n",
    "        self.z_U_sample = self._draw_sample(self.z_U_mu, self.z_U_lsgms)\n",
    "\n",
    "        self.decoder_U_out = self._rnndecoder(self.xs_U, tf.concat([self.z_U_sample, self.y_U_sample], 1), self.dim_h, self.dim_x, reuse = True)\n",
    "        self.x_U_recon = tf.nn.softmax(self.decoder_U_out)\n",
    "\n",
    "        self.encoder_U2_out = self._rnnencoder(self.x_U, self.y_U_mu, self.dim_h, 2*self.dim_z, reuse = True)\n",
    "        self.z_U2_mu, self.z_U2_lsgms = tf.split(self.encoder_U2_out, [self.dim_z, self.dim_z], 1)\n",
    "        \n",
    "        self.decoder_DU_out = self._rnndecoder(self.xs_U, tf.concat([self.z_U2_mu, self.y_U_mu], 1), self.dim_h, self.dim_x, reuse = True)\n",
    "        self.x_DU_recon = tf.nn.softmax(self.decoder_DU_out)\n",
    "\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.session = tf.Session()\n",
    "        \n",
    "\n",
    "    def train(self, trnX_L, trnXs_L, trnY_L, trnX_U, trnXs_U, valX_L, valXs_L, valY_L, valX_U, valXs_U):\n",
    "\n",
    "        self.mu_prior=np.mean(trnY_L,0)   \n",
    "        self.cov_prior=np.cov(trnY_L.T)     \n",
    "\n",
    "        self.tf_mu_prior=tf.constant(self.mu_prior, shape=[1, self.dim_y], dtype=tf.float32)   \n",
    "        self.tf_cov_prior=tf.constant(self.cov_prior, shape=[self.dim_y, self.dim_y], dtype=tf.float32)\n",
    "\n",
    "\n",
    "        # objective functions\n",
    "        objL = self._obj_L()\n",
    "        objU = self._obj_U()\n",
    "        objYpred_MSE = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(self.y_L, self.y_L_mu), 1))\n",
    "        \n",
    "        objL_val = - tf.reduce_mean(- tf.reduce_sum(self.cross_entropy(tf.layers.flatten(self.x_L), tf.layers.flatten(self.x_DL_recon)), 1))\n",
    "        objU_val = - tf.reduce_mean(- tf.reduce_sum(self.cross_entropy(tf.layers.flatten(self.x_U), tf.layers.flatten(self.x_DU_recon)), 1))\n",
    "\n",
    "        batch_size_L=int(self.batch_size*len(trnX_L)/(len(trnX_L)+len(trnX_U)))\n",
    "        batch_size_U=int(self.batch_size*len(trnX_U)/(len(trnX_L)+len(trnX_U)))\n",
    "        n_batch=int(len(trnX_L)/batch_size_L)\n",
    "        \n",
    "        batch_size_val_L=int(len(valX_L)/10)\n",
    "        batch_size_val_U=int(len(valX_U)/10)\n",
    "\n",
    "        cost = (objL * float(batch_size_L) + objU * float(batch_size_U))/float(batch_size_L+batch_size_U) + float(batch_size_L)/float(batch_size_L+batch_size_U) * (self.beta * objYpred_MSE)\n",
    "        cost_val = objYpred_MSE\n",
    "        train_op = tf.train.AdamOptimizer().minimize(cost)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        # training\n",
    "        val_log=np.zeros(300)\n",
    "        for epoch in range(300):\n",
    "            [trnX_L, trnXs_L, trnY_L]=self._permutation([trnX_L, trnXs_L, trnY_L])\n",
    "            [trnX_U, trnXs_U]=self._permutation([trnX_U, trnXs_U])\n",
    "\n",
    "            for i in range(n_batch):\n",
    "                start_L=i*batch_size_L\n",
    "                end_L=start_L+batch_size_L\n",
    "                \n",
    "                start_U=i*batch_size_U\n",
    "                end_U=start_U+batch_size_U\n",
    "\n",
    "                trn_res = self.session.run([train_op, cost, objL, objU, objYpred_MSE],\n",
    "                                      feed_dict = {self.x_L: trnX_L[start_L:end_L], self.xs_L: trnXs_L[start_L:end_L], self.y_L: trnY_L[start_L:end_L],\n",
    "                                      self.x_U: trnX_U[start_U:end_U], self.xs_U: trnXs_U[start_U:end_U]})\n",
    "\n",
    "            val_res = []\n",
    "            for i in range(10):\n",
    "                start_L=i*batch_size_val_L\n",
    "                end_L=start_L+batch_size_val_L\n",
    "                \n",
    "                start_U=i*batch_size_val_U\n",
    "                end_U=start_U+batch_size_val_U\n",
    "            \n",
    "                val_res.append(self.session.run([cost_val, objL_val, objU_val, objYpred_MSE],\n",
    "                                  feed_dict = {self.x_L: valX_L[start_L:end_L], self.xs_L: valXs_L[start_L:end_L], self.y_L: valY_L[start_L:end_L],\n",
    "                                  self.x_U: valX_U[start_U:end_U], self.xs_U: valXs_U[start_U:end_U]}))\n",
    "            \n",
    "            val_res=np.mean(val_res,axis=0)\n",
    "            print(epoch, ['Training', 'cost_trn', trn_res[1]])\n",
    "            print('---', ['Validation', 'cost_val', val_res[0]])\n",
    "\n",
    "            val_log[epoch] = val_res[0]\n",
    "            if epoch > 20 and np.min(val_log[0:epoch-10]) * 0.99 < np.min(val_log[epoch-10:epoch+1]):\n",
    "                print('---termination condition is met')\n",
    "                break\n",
    "\n",
    "\n",
    "    def predict(self, x_input):\n",
    "\n",
    "        return self.session.run(self.y_U_mu, feed_dict = {self.x_U: x_input})\n",
    "\n",
    "\n",
    "    def latent(self, x_input, y_input):\n",
    "\n",
    "        return self.session.run(self.z_L_mu, feed_dict = {self.x_L: x_input, self.y_L: y_input})\n",
    "\n",
    "\n",
    "    def sampling_unconditional(self): \n",
    "       \n",
    "        sample_z=np.random.randn(1, self.dim_z)\n",
    "        sample_y=np.random.multivariate_normal(self.mu_prior, self.cov_prior, 1)      \n",
    "          \n",
    "        sample_smiles=self.beam_search(sample_z, sample_y, k=5)\n",
    "\n",
    "        return sample_smiles\n",
    "        \n",
    "    \n",
    "    def sampling_conditional(self, yid, ytarget):\n",
    "    \n",
    "        def random_cond_normal(yid, ytarget):\n",
    "\n",
    "            id2=[yid]\n",
    "            id1=np.setdiff1d([0,1,2],id2)\n",
    "        \n",
    "            mu1=self.mu_prior[id1]\n",
    "            mu2=self.mu_prior[id2]\n",
    "            \n",
    "            cov11=self.cov_prior[id1][:,id1]\n",
    "            cov12=self.cov_prior[id1][:,id2]\n",
    "            cov22=self.cov_prior[id2][:,id2]\n",
    "            cov21=self.cov_prior[id2][:,id1]\n",
    "            \n",
    "            cond_mu=np.transpose(mu1.T+np.matmul(cov12, np.linalg.inv(cov22)) * (ytarget-mu2))[0]\n",
    "            cond_cov=cov11 - np.matmul(np.matmul(cov12, np.linalg.inv(cov22)), cov21)\n",
    "            \n",
    "            marginal_sampled=np.random.multivariate_normal(cond_mu, cond_cov, 1)\n",
    "            \n",
    "            tst=np.zeros(3)\n",
    "            tst[id1]=marginal_sampled\n",
    "            tst[id2]=ytarget\n",
    "            \n",
    "            return np.asarray([tst])\n",
    "\n",
    "        sample_z=np.random.randn(1, self.dim_z)\n",
    "        sample_y=random_cond_normal(yid, ytarget) \n",
    "          \n",
    "        sample_smiles=self.beam_search(sample_z, sample_y, k=5)\n",
    "            \n",
    "        return sample_smiles\n",
    "\n",
    "\n",
    "    def beam_search(self, z_input, y_input, k=5):\n",
    "\n",
    "        def reconstruct(xs_input, z_sample, y_input):\n",
    "\n",
    "            return self.session.run(self.x_G_recon, feed_dict = {self.xs_L: xs_input, self.z_G: z_sample, self.y_L: y_input})\n",
    "        \n",
    "        \n",
    "        cands=np.asarray([np.zeros((1, self.seqlen_x, self.dim_x), dtype=np.float32)] )\n",
    "        cands_score=np.asarray([100.])\n",
    "        \n",
    "        for i in range(self.seqlen_x-1):\n",
    "        \n",
    "            cands2=[]\n",
    "            cands2_score=[]\n",
    "\n",
    "            for j, samplevec in enumerate(cands):\n",
    "                o = reconstruct(samplevec, z_input, y_input)\n",
    "                sampleidxs = np.argsort(-o[0,i])[:k]\n",
    "                \n",
    "                for sampleidx in sampleidxs: \n",
    "                    \n",
    "                    samplevectt=np.copy(samplevec)\n",
    "                    samplevectt[0, i+1, sampleidx] = 1.\n",
    "                    \n",
    "                    cands2.append(samplevectt)\n",
    "                    cands2_score.append(cands_score[j] * o[0,i,sampleidx])\n",
    "                    \n",
    "            cands2_score=np.asarray(cands2_score)\n",
    "            cands2=np.asarray(cands2)\n",
    "            \n",
    "            kbestid = np.argsort(-cands2_score)[:k]\n",
    "            cands=np.copy(cands2[kbestid])\n",
    "            cands_score=np.copy(cands2_score[kbestid])\n",
    "            \n",
    "            if np.sum([np.argmax(c[0][i+1]) for c in cands])==0:\n",
    "                break\n",
    "\n",
    "        sampletxt = ''.join([self.int_to_char[np.argmax(t)] for t in cands[0,0]]).strip()\n",
    "\n",
    "        return sampletxt\n",
    "\n",
    "\n",
    "    def _obj_L(self):\n",
    "\n",
    "        L_log_lik = - tf.reduce_sum(self.cross_entropy(tf.layers.flatten(self.x_L), tf.layers.flatten(self.x_L_recon)), 1)\n",
    "        L_log_prior_y = self.noniso_logpdf(self.y_L)\n",
    "        L_KLD_z = self.iso_KLD(self.z_L_mu, self.z_L_lsgms)\n",
    "\n",
    "        objL = - tf.reduce_mean(L_log_lik + L_log_prior_y - L_KLD_z)\n",
    "        \n",
    "        return objL\n",
    "\n",
    "\n",
    "    def _obj_U(self):\n",
    "\n",
    "        U_log_lik = - tf.reduce_sum(self.cross_entropy(tf.layers.flatten(self.x_U), tf.layers.flatten(self.x_U_recon)), 1)\n",
    "        U_KLD_y = self.noniso_KLD(self.y_U_mu, self.y_U_lsgms)\n",
    "        U_KLD_z = self.iso_KLD(self.z_U_mu, self.z_U_lsgms)\n",
    "\n",
    "        objU = - tf.reduce_mean(U_log_lik - U_KLD_y - U_KLD_z)\n",
    "        \n",
    "        return objU\n",
    "\n",
    "\n",
    "    def cross_entropy(self, x, y, const = 1e-10):\n",
    "        return - ( x*tf.log(tf.clip_by_value(y, const, 1.0))+(1.0-x)*tf.log(tf.clip_by_value(1.0-y, const, 1.0)) )\n",
    "\n",
    "\n",
    "    def iso_KLD(self, mu, log_sigma_sq):\n",
    "        return tf.reduce_sum( - 0.5 * (1.0 + log_sigma_sq - tf.square(mu) - tf.exp(log_sigma_sq) ), 1)\n",
    "\n",
    "\n",
    "    def noniso_logpdf(self, x):\n",
    "        return - 0.5 * (float(self.cov_prior.shape[0]) * np.log(2.*np.pi) +  np.log(np.linalg.det(self.cov_prior))\n",
    "                        + tf.reduce_sum( tf.multiply( tf.matmul( tf.subtract(x, self.tf_mu_prior), tf.matrix_inverse(self.tf_cov_prior) ), tf.subtract(x, self.tf_mu_prior) ), 1) )\n",
    "\n",
    "\n",
    "    def noniso_KLD(self, mu, log_sigma_sq):\n",
    "        return 0.5 * ( tf.trace( tf.scan(lambda a, x: tf.matmul(tf.matrix_inverse(self.tf_cov_prior), x), tf.matrix_diag(tf.exp(log_sigma_sq)) ) ) \n",
    "                      + tf.reduce_sum( tf.multiply( tf.matmul( tf.subtract(self.tf_mu_prior, mu), tf.matrix_inverse(self.tf_cov_prior) ), tf.subtract(self.tf_mu_prior, mu) ), 1)\n",
    "                      - float(self.cov_prior.shape[0]) + np.log(np.linalg.det(self.cov_prior)) - tf.reduce_sum(log_sigma_sq, 1) )  \n",
    "\n",
    "\n",
    "    def _permutation(self, set):\n",
    "\n",
    "        permid=np.random.permutation(len(set[0]))\n",
    "        for i in range(len(set)):\n",
    "            set[i]=set[i][permid]\n",
    "\n",
    "        return set\n",
    "\n",
    "\n",
    "    def _draw_sample(self, mu, lsgms):\n",
    "\n",
    "        epsilon = tf.random_normal((tf.shape(mu)), 0, 1)\n",
    "        sample = tf.add(mu, tf.multiply(tf.exp(0.5*lsgms), epsilon))\n",
    "\n",
    "        return sample \n",
    "\n",
    "\n",
    "    def _rnnpredictor(self, x, dim_h, dim_y, reuse=False):\n",
    "\n",
    "        with tf.variable_scope('rnnpredictor', reuse=reuse):\n",
    "\n",
    "            cell_fw = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(dim_h) for _ in range(self.n_hidden)])\n",
    "            cell_bw = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(dim_h) for _ in range(self.n_hidden)])\n",
    "            init_state_fw = cell_fw.zero_state(tf.shape(x)[0], tf.float32)\n",
    "            init_state_bw = cell_bw.zero_state(tf.shape(x)[0], tf.float32)\n",
    "            \n",
    "            _, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x, initial_state_fw=init_state_fw, initial_state_bw=init_state_bw)\n",
    "            res = tf.layers.dense(tf.concat([final_state[0][-1],final_state[1][-1]], 1), dim_y)\n",
    "            \n",
    "            \n",
    "        return res\n",
    "\n",
    "\n",
    "    def _rnnencoder(self, x, st, dim_h, dim_y, reuse=False):\n",
    "\n",
    "        with tf.variable_scope('rnnencoder', reuse=reuse):\n",
    "\n",
    "            cell_fw = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(dim_h) for _ in range(self.n_hidden)])\n",
    "            cell_bw = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(dim_h) for _ in range(self.n_hidden)])\n",
    "            init_state_fw = tf.layers.dense(st, dim_h, activation = tf.nn.sigmoid)\n",
    "            init_state_bw = tf.layers.dense(st, dim_h, activation = tf.nn.sigmoid)\n",
    "            peek_in = tf.layers.dense(st, self.dim_x, activation = tf.nn.sigmoid)\n",
    "            peek = tf.reshape(tf.tile(peek_in, [1, self.seqlen_x]), [-1, self.seqlen_x, self.dim_x])\n",
    "            \n",
    "            _, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, tf.concat([x,peek],2),\n",
    "                                initial_state_fw=tuple([init_state_fw]*self.n_hidden), initial_state_bw=tuple([init_state_bw]*self.n_hidden))\n",
    "            res = tf.layers.dense(tf.concat([final_state[0][-1],final_state[1][-1]], 1), dim_y)\n",
    "            \n",
    "            \n",
    "        return res\n",
    "\n",
    "\n",
    "    def _rnndecoder(self, x, st, dim_h, dim_y, reuse=False):\n",
    "\n",
    "        with tf.variable_scope('rnndecoder', reuse=reuse):\n",
    "        \n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(dim_h) for _ in range(self.n_hidden)])\n",
    "            init_state = tf.layers.dense(st, dim_h, activation = tf.nn.sigmoid)\n",
    "            peek_in = tf.layers.dense(st, self.dim_x, activation = tf.nn.sigmoid)\n",
    "            peek = tf.reshape(tf.tile(peek_in, [1, self.seqlen_x]), [-1, self.seqlen_x, self.dim_x])\n",
    "\n",
    "            rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, tf.concat([x,peek],2), initial_state=tuple([init_state]*self.n_hidden))\n",
    "            res = tf.layers.dense(rnn_outputs, dim_y)\n",
    "\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: data preparation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcolmenarejo/anaconda3/envs/tf35/lib/python3.5/site-packages/ipykernel_launcher.py:28: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/gcolmenarejo/anaconda3/envs/tf35/lib/python3.5/site-packages/ipykernel_launcher.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: model training\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pds\n",
    "from preprocessing import smiles_to_seq, vectorize\n",
    "import SSVAE\n",
    "\n",
    "from preprocessing import get_property, canonocalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# pre-defined parameters\n",
    "frac=0.5\n",
    "beta=10000.\n",
    "char_set=[' ','1','2','3','4','5','6','7','8','9','-','#','(',')','[',']','+','=','B','Br','c','C','Cl','F','H','I','N','n','O','o','P','p','S','s','Si','Sn']\n",
    "data_uri='./data/ZINC_310k.csv'\n",
    "save_uri='./zinc_model.ckpt'\n",
    "\n",
    "ntrn=300000\n",
    "frac_val=0.05\n",
    "ntst=10000\n",
    "\n",
    "\n",
    "# data preparation\n",
    "print('::: data preparation')\n",
    "\n",
    "smiles = pds.read_csv(data_uri).as_matrix()[:ntrn+ntst,0] #0: SMILES\n",
    "Y = np.asarray(pds.read_csv(data_uri).as_matrix()[:ntrn+ntst,1:], dtype=np.float32) # 1: MolWT, 2: LogP, 3: QED \n",
    "\n",
    "list_seq = smiles_to_seq(smiles, char_set)\n",
    "Xs, X=vectorize(list_seq, char_set)\n",
    "\n",
    "tstX=X[-ntst:]\n",
    "tstXs=Xs[-ntst:]\n",
    "tstY=Y[-ntst:]\n",
    "\n",
    "X=X[:ntrn]\n",
    "Xs=Xs[:ntrn]\n",
    "Y=Y[:ntrn]\n",
    "\n",
    "nL=int(len(Y)*frac)\n",
    "nU=len(Y)-nL\n",
    "nL_trn=int(nL*(1-frac_val))\n",
    "nL_val=nL-nL_trn\n",
    "nU_trn=int(nU*(1-frac_val))\n",
    "nU_val=nU-nU_trn\n",
    "perm_id=np.random.permutation(len(Y))\n",
    "\n",
    "trnX_L=X[perm_id[:nL_trn]]\n",
    "trnXs_L=Xs[perm_id[:nL_trn]]\n",
    "trnY_L=Y[perm_id[:nL_trn]]\n",
    "\n",
    "valX_L=X[perm_id[nL_trn:nL_trn+nL_val]]\n",
    "valXs_L=Xs[perm_id[nL_trn:nL_trn+nL_val]]\n",
    "valY_L=Y[perm_id[nL_trn:nL_trn+nL_val]]\n",
    "\n",
    "trnX_U=X[perm_id[nL_trn+nL_val:nL_trn+nL_val+nU_trn]]\n",
    "trnXs_U=Xs[perm_id[nL_trn+nL_val:nL_trn+nL_val+nU_trn]]\n",
    "\n",
    "valX_U=X[perm_id[nL_trn+nL_val+nU_trn:]]\n",
    "valXs_U=Xs[perm_id[nL_trn+nL_val+nU_trn:]]\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "scaler_Y.fit(Y)\n",
    "trnY_L=scaler_Y.transform(trnY_L)\n",
    "valY_L=scaler_Y.transform(valY_L)\n",
    "\n",
    "\n",
    "## model training\n",
    "print('::: model training')\n",
    "\n",
    "seqlen_x = X.shape[1]\n",
    "dim_x = X.shape[2]\n",
    "dim_y = Y.shape[1]\n",
    "dim_z = 100\n",
    "dim_h = 250\n",
    "\n",
    "n_hidden = 3\n",
    "batch_size = 200\n",
    "\n",
    "model = SSVAE.Model(seqlen_x = seqlen_x, dim_x = dim_x, dim_y = dim_y, dim_z = dim_z, dim_h = dim_h,\n",
    "                    n_hidden = n_hidden, batch_size = batch_size, beta = float(beta), char_set = char_set)\n",
    "\n",
    "with model.session:\n",
    "    model.train(trnX_L=trnX_L, trnXs_L=trnXs_L, trnY_L=trnY_L, trnX_U=trnX_U, trnXs_U=trnXs_U,\n",
    "                valX_L=valX_L, valXs_L=valXs_L, valY_L=valY_L, valX_U=valX_U, valXs_U=valXs_U)\n",
    "    model.saver.save(model.session, save_uri)\n",
    "\n",
    "    ## property prediction performance\n",
    "    tstY_hat=scaler_Y.inverse_transform(model.predict(tstX))\n",
    "\n",
    "    for j in range(dim_y):\n",
    "        print([j, mean_absolute_error(tstY[:,j], tstY_hat[:,j])])\n",
    "        \n",
    "        \n",
    "    ## unconditional generation\n",
    "    for t in range(10):\n",
    "        smi = model.sampling_unconditional()\n",
    "        print([t, smi, get_property(smi)])\n",
    "    \n",
    "    ## conditional generation (e.g. MolWt=250)\n",
    "    yid = 0\n",
    "    ytarget = 250.\n",
    "    ytarget_transform = (ytarget-scaler_Y.mean_[yid])/np.sqrt(scaler_Y.var_[yid])\n",
    "    \n",
    "    for t in range(10):\n",
    "        smi = model.sampling_conditional(yid, ytarget_transform)\n",
    "        print([t, smi, get_property(smi)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
